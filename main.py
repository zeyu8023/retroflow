import socket
import struct
import threading
import time
import json
import logging
import os
import sqlite3
from flask import Flask, jsonify, send_file, request
import docker

# --- é…ç½®åŒº ---
HOST_SERVICES = {
    8096: "Emby (åª’ä½“)",
    8920: "Emby (HTTPS)",
    10308: "RetroFlow (æœ¬æœåŠ¡)",
    80: "Nginx (Web)",
    443: "Nginx (SSL)",
    # åœ¨è¿™é‡Œç»§ç»­æ·»åŠ ä½ çš„ç«¯å£...
}

# --- å…¨å±€æ•°æ® ---
# å®æ—¶ç´¯è®¡å€¼ (é‡å¯å½’é›¶)
stats_store = {}
# ä¸Šä¸€æ¬¡ä¿å­˜æ—¶çš„ç´¯è®¡å€¼ (ç”¨äºè®¡ç®—æ¯åˆ†é’Ÿå¢é‡)
last_saved_stats = {}
lock = threading.Lock()

# åˆå§‹åŒ– Flask å’Œ Docker
app = Flask(__name__)
try:
    docker_client = docker.from_env()
except:
    docker_client = None

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

# --- æ•°æ®åº“ç®¡ç† ---
DB_PATH = 'data/traffic.db'

def init_db():
    if not os.path.exists('data'):
        os.makedirs('data')
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    # åˆ›å»ºå†å²è¡¨ï¼šæ—¶é—´æˆ³, å®¹å™¨å, ä¸Šä¼ å¢é‡, ä¸‹è½½å¢é‡
    c.execute('''CREATE TABLE IF NOT EXISTS history 
                 (timestamp INTEGER, name TEXT, upload INTEGER, download INTEGER)''')
    # åˆ›å»ºç´¢å¼•åŠ é€ŸæŸ¥è¯¢
    c.execute('''CREATE INDEX IF NOT EXISTS idx_time ON history (timestamp)''')
    conn.commit()
    conn.close()

def save_history_task():
    """åå°ä»»åŠ¡ï¼šæ¯åˆ†é’Ÿå°†å¢é‡æ•°æ®å†™å…¥æ•°æ®åº“"""
    while True:
        time.sleep(60) # æ¯60ç§’ä¿å­˜ä¸€æ¬¡
        timestamp = int(time.time())
        
        with lock:
            conn = sqlite3.connect(DB_PATH)
            c = conn.cursor()
            
            for name, data in stats_store.items():
                current_up = data['upload']
                current_down = data['download']
                
                # è·å–ä¸Šä¸€æ¬¡ä¿å­˜çš„å€¼
                last = last_saved_stats.get(name, {'upload': 0, 'download': 0})
                
                # è®¡ç®—è¿™ä¸€åˆ†é’Ÿå†…çš„å¢é‡ (Delta)
                delta_up = current_up - last['upload']
                delta_down = current_down - last['download']
                
                # åªæœ‰å½“æœ‰æµé‡äº§ç”Ÿæ—¶æ‰è®°å½•ï¼ŒèŠ‚çœç©ºé—´
                if delta_up > 0 or delta_down > 0:
                    c.execute("INSERT INTO history VALUES (?, ?, ?, ?)", 
                              (timestamp, name, delta_up, delta_down))
                
                # æ›´æ–°â€œä¸Šä¸€æ¬¡â€çš„è®°å½•
                last_saved_stats[name] = {'upload': current_up, 'download': current_down}
            
            conn.commit()
            conn.close()
            logging.info(f"ğŸ’¾ [DB] å·²å½’æ¡£å†å²æ•°æ® - {timestamp}")

# --- è¾…åŠ©å‡½æ•° ---
def ensure_stats(name, net_type):
    if name not in stats_store:
        stats_store[name] = {"name": name, "type": net_type, "upload": 0, "download": 0}

# --- æŠ“åŒ…æ¨¡å— (Host) ---
def start_sniffer(interface="eth0"):
    logging.info(f"ğŸ•¸ï¸ [Sniffer] å¼€å§‹ç›‘å¬ {interface}...")
    try:
        sock = socket.socket(socket.AF_PACKET, socket.SOCK_RAW, socket.ntohs(0x0003))
        sock.bind((interface, 0))
    except Exception as e:
        logging.error(f"âŒ æŠ“åŒ…å¤±è´¥: {e}")
        return

    while True:
        try:
            raw_data, _ = sock.recvfrom(65535)
            packet_len = len(raw_data)
            eth_proto = struct.unpack("!6s6sH", raw_data[:14])[2]
            
            if eth_proto != 0x0800: continue # åªçœ‹ IPv4

            ip_header = raw_data[14:34]
            iph = struct.unpack('!BBHHHBBH4s4s', ip_header)
            protocol = iph[6]
            if protocol != 6 and protocol != 17: continue # åªçœ‹ TCP/UDP

            ihl = (iph[0] & 0xF) * 4
            transport_offset = 14 + ihl
            transport_header = raw_data[transport_offset:transport_offset+4]
            src_port, dst_port = struct.unpack('!HH', transport_header)

            with lock:
                if dst_port in HOST_SERVICES:
                    name = HOST_SERVICES[dst_port]
                    ensure_stats(name, "host")
                    stats_store[name]["download"] += packet_len
                if src_port in HOST_SERVICES:
                    name = HOST_SERVICES[src_port]
                    ensure_stats(name, "host")
                    stats_store[name]["upload"] += packet_len
        except:
            continue

# --- Docker ç›‘æ§ (Bridge) ---
def start_docker_monitor():
    while True:
        if not docker_client:
            time.sleep(5)
            continue
        try:
            containers = docker_client.containers.list()
            for c in containers:
                if c.attrs['HostConfig']['NetworkMode'] != 'host':
                    name = c.name
                    with lock:
                        ensure_stats(name, "bridge")
                        # æ¨¡æ‹Ÿæ•°æ®ï¼šPythonç›´æ¥è¯»Bridgeæµé‡è¾ƒéš¾ï¼Œè¿™é‡Œæš‚æ—¶ç•¥è¿‡
                        # é‡ç‚¹æ˜¯è®©å®ƒåœ¨åˆ—è¡¨é‡Œæ˜¾ç¤ºå‡ºæ¥
        except:
            pass
        time.sleep(5)

# --- API è·¯ç”± ---
@app.route('/')
def index():
    return send_file('index.html')

@app.route('/api/realtime')
def get_realtime():
    with lock:
        return jsonify(list(stats_store.values()))

@app.route('/api/history')
def get_history():
    """è·å–å†å²æ•°æ®ç”¨äºç»˜å›¾"""
    # range: 'day' (24h), 'month' (30d), 'year' (12m)
    time_range = request.args.get('range', 'day')
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    
    now = int(time.time())
    data = []

    if time_range == 'day':
        # æŸ¥è¯¢è¿‡å»24å°æ—¶ï¼ŒæŒ‰å°æ—¶èšåˆ
        start_time = now - 24 * 3600
        sql = """
            SELECT strftime('%H:00', datetime(timestamp, 'unixepoch', 'localtime')) as time_label,
                   name, sum(upload), sum(download)
            FROM history WHERE timestamp > ?
            GROUP BY time_label, name
            ORDER BY timestamp
        """
        c.execute(sql, (start_time,))
        
    elif time_range == 'month':
        # æŸ¥è¯¢è¿‡å»30å¤©ï¼ŒæŒ‰å¤©èšåˆ
        start_time = now - 30 * 24 * 3600
        sql = """
            SELECT strftime('%Y-%m-%d', datetime(timestamp, 'unixepoch', 'localtime')) as time_label,
                   name, sum(upload), sum(download)
            FROM history WHERE timestamp > ?
            GROUP BY time_label, name
            ORDER BY timestamp
        """
        c.execute(sql, (start_time,))
    
    rows = c.fetchall()
    conn.close()
    
    # æ ¼å¼åŒ–æ•°æ®ç»™å‰ç«¯
    result = {}
    for row in rows:
        label, name, up, down = row
        if label not in result:
            result[label] = {}
        if name not in result[label]:
            result[label][name] = {'up': 0, 'down': 0}
        result[label][name]['up'] += up
        result[label][name]['down'] += down
        
    return jsonify(result)

if __name__ == '__main__':
    # åˆå§‹åŒ–æ•°æ®åº“
    init_db()
    
    # å¯åŠ¨çº¿ç¨‹
    t1 = threading.Thread(target=start_sniffer, args=("eth0",), daemon=True)
    t1.start()
    t2 = threading.Thread(target=start_docker_monitor, daemon=True)
    t2.start()
    t3 = threading.Thread(target=save_history_task, daemon=True) # æ–°å¢ï¼šå­˜åº“çº¿ç¨‹
    t3.start()

    logging.info("ğŸš€ RetroFlow Pro å·²å¯åŠ¨ :10308")
    app.run(host='0.0.0.0', port=10308)